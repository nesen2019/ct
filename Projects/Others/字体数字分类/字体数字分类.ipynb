{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 字体数字分类"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建随机字体数字图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFont, ImageDraw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAAGUlEQVR4nO3BMQEAAADCoPVP7WENoAAAAG4MIAABt9NlCQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x7F3992D6BB80>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = np.zeros((32,32,3), dtype=np.uint8)\n",
    "Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAACk0lEQVR4nO2Wv2r6UBTHb2MTQs3QIXazJYjtlhr/g7RYqZvOQh6gnTqHPoCLcfAFBMFB0CfoG4guSrY4tL0FBckQpbVek+pvCNyf/4jGrv1Oh9zP+Z5z700OAeBPf/qtjuyXT09Pz8/PSZIkCAIAYBjG29ubruu/LXt8fMzzfLlc1jRtsarBYFAul3met0oeIoIgOp3OYpe63e6BNSRJ2uluSZKknW7rd0DTtKqqXq8XAGAYRqPRqFarCCGEEEmSJycnoijmcjmSJAEAEMKrq6vpdOqg/cfHR9xgqVTayhSLRcw8PT05cAcARKNRnHx3d7eVSSQSmLm9vbU3XL+lYDCI40wmszUnm83iOBAI7NP3fzEMAyG0ukMIiaLo8XgoigIAUBR1dnYmiiJCyAIghAzDOCsANt6ir6+vZrP58vLSbDY/Pz+Xl56fnx27W532er2d76iqqtbODpHP5xsOhzbuw+GQ47gD3QmCaLVaa47z+XztSbvdPsTd6/Xquo5dZrNZo9FIp9OCIKTT6Xq9PpvN8Kqu69Yn6UCFQgHnK4rCsuwawLKsoiiYKRQKDtxpmn5/f7cyJ5PJ5eXlVszv908mEwuDENI0vW+Bh4eHPVvL5/OYdDAtYrEYTksmkzZkPB7H5M3NjQ25Mirm8zmOTdO0SVssFjj++fnZt0AoFMLx8sDZ1PJqOBy2IVfEMMzHx4e18e/v7+vr662YIAjLl+xsHMmyjA8XIVSr1VKpVCQSiUQisVjs/v6+VqsZhoEZWZYduAMAOI4bj8cbo2G7RqPRIQPD7/fjg7IRhNDn8zl2t0RRFM/zlUql3++v+Q4Gg0qlwvP8nqN0x48Xy7IXFxdut9s0TZfLNZ1OX19fNU3bv9d/LeSuNScYuXQAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x7F392D059940>"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_img(text, nums=1, shape=(32,32,3)):\n",
    "    # img = np.random.randint(0, 255, shape, dtype=np.uint8)\n",
    "    img = np.zeros(shape, dtype=np.uint8)\n",
    "    img = cv2.putText(img, f\"{text}\", (3, shape[1] - 3), cv2.FONT_HERSHEY_SCRIPT_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    return img \n",
    "\n",
    "Image.fromarray(get_img(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from torch.utils.data import Dataset \n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FontNumberDataset(Dataset):\n",
    "    def __init__(self, length=60000):\n",
    "        self.length = length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label = random.randint(0, 9)\n",
    "        img = get_img(label)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        gray = torch.Tensor(gray)\n",
    "        gray = gray.gray = gray.unsqueeze(0)\n",
    "        return gray, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 10])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray = torch.Tensor(np.random.randint(0, 255, (10,10)))\n",
    "gray = gray.unsqueeze(0)\n",
    "gray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader = DataLoader(FontNumberDataset(), batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(FontNumberDataset(), batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 32, 32])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 9\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建PyTorch模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "LeNet5(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): Tanh()\n",
      "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): Tanh()\n",
      "    (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (6): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (7): Tanh()\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes=-1):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1), nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1), nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            nn.Conv2d(16, 120, kernel_size=5, stride=1), nn.Tanh())\n",
    "        if self.num_classes > 0:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(120, 84),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(84, num_classes),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.features(x)\n",
    "        if self.num_classes > 0:\n",
    "            x = self.classifier(x.squeeze())\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = LeNet5(10).to(device)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.306608  [   64/60000]\n",
      "loss: 2.316438  [ 6464/60000]\n",
      "loss: 2.319332  [12864/60000]\n",
      "loss: 2.311575  [19264/60000]\n",
      "loss: 2.329077  [25664/60000]\n",
      "loss: 2.306381  [32064/60000]\n",
      "loss: 2.321919  [38464/60000]\n",
      "loss: 2.330587  [44864/60000]\n",
      "loss: 2.323778  [51264/60000]\n",
      "loss: 2.309835  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.9%, Avg loss: 2.320163 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.326779  [   64/60000]\n",
      "loss: 2.313706  [ 6464/60000]\n",
      "loss: 2.337355  [12864/60000]\n",
      "loss: 2.317734  [19264/60000]\n",
      "loss: 2.334779  [25664/60000]\n",
      "loss: 2.306442  [32064/60000]\n",
      "loss: 2.330477  [38464/60000]\n",
      "loss: 2.313225  [44864/60000]\n",
      "loss: 2.323377  [51264/60000]\n",
      "loss: 2.317936  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.1%, Avg loss: 2.319780 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.322984  [   64/60000]\n",
      "loss: 2.319089  [ 6464/60000]\n",
      "loss: 2.345253  [12864/60000]\n",
      "loss: 2.301015  [19264/60000]\n",
      "loss: 2.313772  [25664/60000]\n",
      "loss: 2.300312  [32064/60000]\n",
      "loss: 2.325882  [38464/60000]\n",
      "loss: 2.315132  [44864/60000]\n",
      "loss: 2.327062  [51264/60000]\n",
      "loss: 2.304287  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss: 2.319786 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.319030  [   64/60000]\n",
      "loss: 2.309652  [ 6464/60000]\n",
      "loss: 2.336704  [12864/60000]\n",
      "loss: 2.315458  [19264/60000]\n",
      "loss: 2.309846  [25664/60000]\n",
      "loss: 2.315264  [32064/60000]\n",
      "loss: 2.300007  [38464/60000]\n",
      "loss: 2.326619  [44864/60000]\n",
      "loss: 2.314146  [51264/60000]\n",
      "loss: 2.321883  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss: 2.319846 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.329321  [   64/60000]\n",
      "loss: 2.323154  [ 6464/60000]\n",
      "loss: 2.317477  [12864/60000]\n",
      "loss: 2.329167  [19264/60000]\n",
      "loss: 2.317223  [25664/60000]\n",
      "loss: 2.298921  [32064/60000]\n",
      "loss: 2.336780  [38464/60000]\n",
      "loss: 2.316171  [44864/60000]\n",
      "loss: 2.333612  [51264/60000]\n",
      "loss: 2.318956  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.9%, Avg loss: 2.320493 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 28, 28]) torch.float32 cuda:0\n"
     ]
    }
   ],
   "source": [
    "for X, y in test_dataloader:\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    print(X.shape, X.dtype, X.device)\n",
    "    model.to(device)\n",
    "    model(X)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "torch.Size([1, 1, 32, 32]) torch.float32 cuda:0\n",
      "tensor([-0.1024,  0.0019,  0.1228, -0.0972,  0.0768, -0.0446,  0.0355,  0.0413,\n",
      "        -0.1128, -0.1799], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "img = get_img(2)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "x = torch.from_numpy(np.array(gray, dtype=np.float32))\n",
    "x = x.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "x = x.to(device)\n",
    "model = model.to(device)\n",
    "print(x.shape, x.dtype, x.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    print(pred)\n",
    "    print(pred.argmax(0))\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     pred = model(x)\n",
    "#     predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "#     print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0473, -0.0488,  0.1077, -0.0645,  0.0251, -0.0322,  0.0493, -0.0204,\n",
      "         -0.0721, -0.0746],\n",
      "        [-0.0473, -0.0488,  0.1077, -0.0645,  0.0251, -0.0322,  0.0493, -0.0204,\n",
      "         -0.0721, -0.0746]], device='cuda:0', grad_fn=<AddmmBackward0>) tensor([1, 1], device='cuda:0')\n",
      "tensor([False, False], device='cuda:0')\n",
      "tensor([[-0.1556,  0.0293,  0.0783, -0.1258,  0.1074, -0.0619,  0.0322,  0.0994,\n",
      "         -0.1469, -0.1915],\n",
      "        [-0.1024,  0.0019,  0.1228, -0.0972,  0.0768, -0.0446,  0.0355,  0.0413,\n",
      "         -0.1128, -0.1799]], device='cuda:0', grad_fn=<AddmmBackward0>) tensor([9, 2], device='cuda:0')\n",
      "tensor([False,  True], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "tdata = DataLoader(FontNumberDataset(4), batch_size=2, shuffle=True)\n",
    "\n",
    "for batch, (X, y) in enumerate(tdata):\n",
    "    X, y = X.to(device), y.to(device)\n",
    "\n",
    "    # Compute prediction error\n",
    "    pred = model(X)\n",
    "    print(pred, y)\n",
    "\n",
    "    print(pred.argmax(1) == y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4238c57844cc908b76449bc4ddf9899590ca8e7952f36c82afaad1c379a56db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
